{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting ðŸ¤— Hugging Face models to ONNX\n",
    "You can use both Python modules used as a command-line tool or use the libraries directly. This notebook will explore how to export using the `transformers.onnx` module as a CLI and then consume that resulting model with Hugging Face and the `onnxruntime` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the features\n",
    "List the available features for a specific model. You can then use that feature when exporting to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['default', 'masked-lm', 'sequence-classification', 'multiple-choice', 'token-classification', 'question-answering']\n"
     ]
    }
   ],
   "source": [
    "from transformers.onnx.features import FeaturesManager\n",
    "\n",
    "distilbert_features = list(FeaturesManager.get_supported_features_for_model_type(\"distilbert\").keys())\n",
    "print(distilbert_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Framework not requested. Using torch to export to ONNX.\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 629/629 [00:00<00:00, 99.4kB/s]\n",
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268M/268M [00:00<00:00, 324MB/s]\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48.0/48.0 [00:00<00:00, 11.8kB/s]\n",
      "vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 2.39MB/s]\n",
      "Using framework PyTorch: 2.4.1\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:215: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/onnx/_internal/onnx_proto_utils.py\", line 220, in _add_onnxscript_fn\n",
      "    import onnx\n",
      "ModuleNotFoundError: No module named 'onnx'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/onnx/__main__.py\", line 242, in <module>\n",
      "    main()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/onnx/__main__.py\", line 234, in main\n",
      "    export_with_transformers(args)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/onnx/__main__.py\", line 167, in export_with_transformers\n",
      "    onnx_inputs, onnx_outputs = export(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/onnx/convert.py\", line 312, in export\n",
      "    return export_pytorch(preprocessor, model, config, opset, output, tokenizer=tokenizer, device=device)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/onnx/convert.py\", line 169, in export_pytorch\n",
      "    onnx_export(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/onnx/utils.py\", line 551, in export\n",
      "    _export(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/onnx/utils.py\", line 1722, in _export\n",
      "    proto = onnx_proto_utils._add_onnxscript_fn(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/onnx/_internal/onnx_proto_utils.py\", line 222, in _add_onnxscript_fn\n",
      "    raise errors.OnnxExporterError(\"Module onnx is not installed!\") from e\n",
      "torch.onnx.errors.OnnxExporterError: Module onnx is not installed!\n"
     ]
    }
   ],
   "source": [
    "!python -m transformers.onnx --model=distilbert-base-uncased-finetuned-sst-2-english \\\n",
    "                            --feature=question-answering ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and further reading\n",
    "Go through the ONNX models and how they can work with the `onnxruntime` in the [ONNX Zoo](https://github.com/onnx/models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
